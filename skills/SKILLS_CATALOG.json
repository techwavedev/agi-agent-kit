[
  {
    "name": "aws",
    "description": "Comprehensive AWS MCP skill covering ALL AWS services. Use for any AWS-related task - infrastructure, databases, AI/ML, observability, networking, serverless, and more. This single skill provides access to 60+ AWS MCP servers organized by category.",
    "dir_name": "aws",
    "location": "skills/aws/",
    "scripts": [],
    "references": [
      "common_patterns.md",
      "mcp_servers.md"
    ],
    "has_assets": false,
    "parent": null
  },
  {
    "name": "aws-terraform",
    "description": "AWS infrastructure deployments using Terraform and Terragrunt. Use for any task involving: (1) Writing, validating, or deploying Terraform/HCL code for AWS, (2) Security scanning with Checkov, (3) AWS provider documentation lookup, (4) Terraform Registry module analysis, (5) Terragrunt multi-environment orchestration, (6) Infrastructure as Code best practices for AWS. Parent skill: aws.",
    "dir_name": "aws-terraform",
    "location": "skills/aws-terraform/",
    "scripts": [
      "configure_mcp.py"
    ],
    "references": [
      "best_practices.md",
      "checkov_reference.md"
    ],
    "has_assets": false,
    "parent": null
  },
  {
    "name": "confluent-kafka",
    "description": "Confluent Kafka specialist for tarball/Ansible custom installations. Expert in updating, maintaining, checking health, troubleshooting, documenting, analyzing metrics, and upgrading Confluent Kafka deployments from 7.x to 8.x versions. Covers KRaft mode (ZooKeeper-less), broker configuration, Schema Registry, Connect, ksqlDB, Control Center, and production-grade operations. Use when working with Confluent Platform installations, migrations to KRaft, performance tuning, health monitoring, and infrastructure-as-code with Ansible.",
    "dir_name": "confluent-kafka",
    "location": "skills/confluent-kafka/",
    "scripts": [
      "kafka_health_check.py",
      "upgrade_preflight.py",
      "validate_config.py"
    ],
    "references": [
      "ansible_playbooks.md",
      "ec_deployment.md",
      "kraft_migration.md",
      "troubleshooting.md",
      "upgrade_7x_to_8x.md"
    ],
    "has_assets": false,
    "parent": "aws"
  },
  {
    "name": "consul",
    "description": "HashiCorp Consul specialist for EKS clusters. Use for Consul service mesh installation, configuration, HA setup, maintenance, updates, upgrades, troubleshooting, and optimization. Covers Consul Connect, intentions, health checks, ACLs, gossip encryption, TLS configuration, federation, and Kubernetes integration via consul-k8s Helm chart. Requires kubectl and helm access to target EKS cluster.",
    "dir_name": "consul",
    "location": "skills/consul/",
    "scripts": [
      "consul_health_report.py",
      "consul_status.py",
      "generate_values.py"
    ],
    "references": [
      "acl_setup.md",
      "ha_config.md",
      "troubleshooting.md",
      "upgrades.md"
    ],
    "has_assets": false,
    "parent": "aws"
  },
  {
    "name": "documentation",
    "description": "\"Automated documentation maintenance and generation skill. Triggers when: (1) Code is added, changed, updated, or deleted in any skill, (2) New scripts or references are created, (3) SKILL.md files are modified, (4) User requests documentation updates, (5) Skills catalog needs regeneration, (6) README or AGENTS.md need updates reflecting code changes. Use for generating technical documentation, updating docs after code changes, producing changelogs, ensuring documentation stays synchronized with the codebase, and maintaining the skills catalog.\"",
    "dir_name": "documentation",
    "location": "skills/documentation/",
    "scripts": [
      "analyze_code.py",
      "detect_changes.py",
      "generate_changelog.py",
      "sync_docs.py",
      "update_skill_docs.py"
    ],
    "references": [
      "best_practices.md"
    ],
    "has_assets": false,
    "parent": null
  },
  {
    "name": "gitlab",
    "description": "GitLab specialist for Kubernetes agent management on EKS clusters. Use for GitLab agent (agentk) installation, configuration, upgrades, GitOps with Flux, CI/CD pipeline integration, project management via API, token management, and troubleshooting connectivity issues. Covers agent registration, Helm deployments, KAS configuration (self-managed on-prem), impersonation, and multi-cluster setups. Requires kubectl/helm access to target EKS cluster and GitLab API token.",
    "dir_name": "gitlab",
    "location": "skills/gitlab/",
    "scripts": [
      "generate_agent_values.py",
      "gitlab_agent_status.py"
    ],
    "references": [
      "agent_installation.md",
      "api_reference.md",
      "gitops_flux.md",
      "troubleshooting.md"
    ],
    "has_assets": false,
    "parent": "aws"
  },
  {
    "name": "jira",
    "description": "\"Jira ticket management skill for creating, updating, and managing issues. Use for: (1) Creating new tickets/issues with custom fields, (2) Updating existing tickets (status, assignee, priority, labels), (3) Adding and updating comments, (4) Logging work time and time tracking, (5) Searching issues with JQL, (6) Managing transitions and workflows, (7) Bulk operations on multiple tickets, (8) Sprint and board management. Supports both MCP server integration and direct REST API calls. Requires JIRA_API_TOKEN and JIRA_URL environment variables.\"",
    "dir_name": "jira",
    "location": "skills/jira/",
    "scripts": [
      "add_comment.py",
      "bulk_log_work.py",
      "create_ticket.py",
      "get_ticket.py",
      "jira_client.py",
      "log_work.py",
      "search_tickets.py",
      "update_comment.py",
      "update_ticket.py"
    ],
    "references": [
      "jql_reference.md"
    ],
    "has_assets": false,
    "parent": null
  },
  {
    "name": "karpenter",
    "description": "Karpenter Kubernetes autoscaler specialist for EKS clusters. Use for troubleshooting, documenting, managing, creating, updating, upgrading Karpenter deployments, and obtaining live cluster information. Covers NodePool/EC2NodeClass configuration, cost optimization, node consolidation, drift detection, Spot interruption handling, and migration from Cluster Autoscaler. Requires kubectl access to target EKS cluster.",
    "dir_name": "karpenter",
    "location": "skills/karpenter/",
    "scripts": [
      "generate_ec2nodeclass.py",
      "generate_nodepool.py",
      "karpenter_status.py"
    ],
    "references": [
      "ec2nodeclasses.md",
      "migration.md",
      "nodepools.md",
      "troubleshooting.md"
    ],
    "has_assets": false,
    "parent": "aws"
  },
  {
    "name": "opensearch",
    "description": "OpenSearch specialist covering querying (Query DSL, SQL), performance optimization, cluster management, monitoring, OpenSearch Dashboards, ML/AI (neural search, embeddings, ML Commons), data ingestion (Logstash, Fluent Bit, Data Prepper), OpenSearch Operator for Kubernetes, and MCP integration. Use for any task involving: (1) Writing or optimizing OpenSearch queries, (2) Index design and mapping, (3) Cluster health and performance tuning, (4) OpenSearch Dashboards visualization, (5) Neural/semantic search with vectors, (6) Log and data ingestion pipelines, (7) Kubernetes deployments with OpenSearch Operator.",
    "dir_name": "opensearch",
    "location": "skills/opensearch/",
    "scripts": [
      "configure_mcp.py"
    ],
    "references": [
      "ml_neural_search.md",
      "operator.md",
      "query_dsl.md"
    ],
    "has_assets": false,
    "parent": null
  },
  {
    "name": "pdf-reader",
    "description": "Extract text from PDF files for manipulation, search, and reference. Use when needing to read PDF content, extract text from documents, search within PDFs, or convert PDF to text for further processing. Supports multiple extraction methods (pdfplumber, PyMuPDF, pdfminer) with automatic fallback.",
    "dir_name": "pdf-reader",
    "location": "skills/pdf-reader/",
    "scripts": [
      "extract_text.py"
    ],
    "references": [
      "pdf_libraries.md"
    ],
    "has_assets": false,
    "parent": null
  },
  {
    "name": "qdrant-memory",
    "description": "\"Intelligent token optimization through Qdrant-powered semantic caching and long-term memory. Use for (1) Semantic Cache - avoid LLM calls entirely for semantically similar queries with 100% token savings, (2) Long-Term Memory - retrieve only relevant context chunks instead of full conversation history with 80-95% context reduction, (3) Hybrid Search - combine vector similarity with keyword filtering for technical queries, (4) Memory Management - store and retrieve conversation memories, decisions, and code patterns with metadata filtering. Triggers when needing to cache responses, remember past interactions, optimize context windows, or implement RAG patterns.\"",
    "dir_name": "qdrant-memory",
    "location": "skills/qdrant-memory/",
    "scripts": [
      "benchmark_token_savings.py",
      "embedding_utils.py",
      "hybrid_search.py",
      "init_collection.py",
      "memory_retrieval.py",
      "semantic_cache.py",
      "test_skill.py"
    ],
    "references": [
      "advanced_patterns.md",
      "collection_schemas.md",
      "complete_guide.md",
      "embedding_models.md"
    ],
    "has_assets": false,
    "parent": null
  },
  {
    "name": "victoriametrics",
    "description": "VictoriaMetrics time-series database specialist covering deployment (bare metal, Docker, EKS/Kubernetes), cluster architecture (vminsert/vmselect/vmstorage), vmagent configuration, performance optimization, capacity planning, troubleshooting, monitoring, and Prometheus migration/compatibility. Use for any task involving: (1) Installing or upgrading VictoriaMetrics (single-node or cluster), (2) vmagent scraping and remote write configuration, (3) Capacity planning and resource optimization, (4) Prometheus to VictoriaMetrics migration with vmctl, (5) High availability and replication setup, (6) Kubernetes/EKS deployments with Helm or Operator, (7) MetricsQL queries and optimization, (8) Troubleshooting performance issues.",
    "dir_name": "victoriametrics",
    "location": "skills/victoriametrics/",
    "scripts": [],
    "references": [
      "kubernetes.md",
      "prometheus_migration.md",
      "troubleshooting.md"
    ],
    "has_assets": false,
    "parent": null
  },
  {
    "name": "webcrawler",
    "description": "\"Documentation harvesting agent for crawling and extracting content from documentation websites. Use for crawling documentation sites and extracting all pages about a subject, building offline knowledge bases from online docs, harvesting API references, tutorials, or guides from documentation portals, creating structured markdown exports from multi-page documentation, and downloading and organizing technical docs for embedding or RAG pipelines. Supports recursive crawling with depth control, content filtering, and structured output.\"",
    "dir_name": "webcrawler",
    "location": "skills/webcrawler/",
    "scripts": [
      "crawl_docs.py",
      "extract_page.py",
      "filter_docs.py"
    ],
    "references": [
      "advanced_crawling.md"
    ],
    "has_assets": false,
    "parent": null
  }
]